#!/usr/bin/env bash
#SBATCH --account=kernlab          ### SLURM account which will be charged for the job
#SBATCH --partition=kern        ### Partition (like a queue in PBS)
#SBATCH --job-name=1000      ### Job Name
#SBATCH --output=1000.out         ### File in which to store job output
#SBATCH --error=1000.err          ### File in which to store job error messages
#SBATCH --time=0-01:00:00       ### Wall clock time limit in Days-HH:MM:SS
#SBATCH --nodes=1               ### Node count required for the job
#SBATCH --ntasks-per-node=1     ### Nuber of tasks to be launched per Node
#SBATCH --cpus-per-task=1       ### Number of cpus (cores) per task
#SBATCH --mem-per-cpu=128G

datadir="/home/vcaudill/kernlab/coevo/MSprome_sim_1000"

#if [ ! -d "/home/vcaudill/kernlab/coevo/pheno_test_1000" ] 
#then
#    mkdir "/home/vcaudill/kernlab/coevo/pheno_test_1000"
#    mkdir "/home/vcaudill/kernlab/coevo/pheno_test_1000/text_files"
#    mkdir "/home/vcaudill/kernlab/coevo/pheno_test_1000/tree_files"
#    exit 
#fi
input='/home/vcaudill/kernlab/coevo/SLiM_script/slimtestvarables.csv'

while IFS="," read -r trial snake_mu_rate newt_mu_rate snake_mu_effect_sd newt_mu_effect_sd more_mu;
do 
echo ${trial}

sbatch runmsprime.srun ${snake_mu_rate} ${newt_mu_rate} ${snake_mu_effect_sd} ${newt_mu_effect_sd} ${datadir} 
	
done < <(tail -n +2 "$input") # skip the header line