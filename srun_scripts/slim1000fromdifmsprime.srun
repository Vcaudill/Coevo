#!/usr/bin/env bash
#SBATCH --account=kernlab          ### SLURM account which will be charged for the job
#SBATCH --partition=kern        ### Partition (like a queue in PBS)
#SBATCH --job-name=1000      ### Job Name
#SBATCH --output=1000.out         ### File in which to store job output
#SBATCH --error=1000.err          ### File in which to store job error messages
#SBATCH --time=0-01:00:00       ### Wall clock time limit in Days-HH:MM:SS
#SBATCH --nodes=1               ### Node count required for the job
#SBATCH --ntasks-per-node=1     ### Nuber of tasks to be launched per Node
#SBATCH --cpus-per-task=1       ### Number of cpus (cores) per task
#SBATCH --mem-per-cpu=128G

outpath="/home/vcaudill/kernlab/coevo/pheno_test_1000_2/text_files"


for myfile in "$1"/*;
do 
arrIN=(${file_name//_/ })

snake_mu_rate=${arrIN[3]}
newt_mu_rate=${arrIN[5]}
snake_mu_effect_sd=${arrIN[7]}
newt_mu_effect_sd_fix==${arrIN[9]}
newt_mu_effect_sd_fix=(${newt_mu_effect_sd_fix//.init/ })
newt_mu_effect_sd=(${newt_mu_effect_sd_fix//=/ })

#FN=$(openssl rand 4 | od -DAn)
#namebase="su_${snake_mu_rate}_nu_${newt_mu_rate}_sue_${snake_mu_effect_sd}_nue_${newt_mu_effect_sd}"
#treename="/home/vcaudill/kernlab/coevo/pheno_test_1000_2/tree_files/"

echo ${snake_mu_rate} ${newt_mu_rate} ${snake_mu_effect_sd} ${newt_mu_effect_sd}

#sbatch runslim.srun ${snake_mu_rate} ${newt_mu_rate} ${snake_mu_effect_sd} ${newt_mu_effect_sd} ${myfile} ${treename} ${outpath} ${namebase} ${FN}

done