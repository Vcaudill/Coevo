#!/bin/bash
#SBATCH --account=kernlab          ### SLURM account which will be charged for the job
#SBATCH --partition=kern        ### Partition (like a queue in PBS)
#SBATCH --job-name=launch      ### Job Name
#SBATCH --output=snakemake-%j.out         ### File in which to store job output
#SBATCH --error=snakemake-%j.out          ### File in which to store job error messages
#SBATCH --time=0-10:10:00       ### Wall clock time limit in Days-HH:MM:SS
#SBATCH --nodes=1               ### Node count required for the job
#SBATCH --ntasks-per-node=1     ### Nuber of tasks to be launched per Node
#SBATCH --cpus-per-task=1       ### Number of cpus (cores) per task
#SBATCH --mem-per-cpu=1G

conda activate coe_proj

snakemake -j 99 --cluster-config cluster_talapas.json --cluster "sbatch -p {cluster.partition} -t {cluster.time} --mem {cluster.mem} -n 1 -c {threads} --gres {cluster.gres} --account={cluster.account}"
#--dryrun

